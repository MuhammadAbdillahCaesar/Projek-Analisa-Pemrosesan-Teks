{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIAQA-XQAv67"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/medical_review.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Step 1: Data Cleaning\n",
        "# Renaming columns for consistency\n",
        "data.columns = data.columns.str.strip().str.replace(' ', '_').str.lower()\n",
        "\n",
        "# Handling missing values\n",
        "for col in ['diagnosis_category', 'diagnosis_sub_category', 'treatment_category',\n",
        "            'treatment_sub_category', 'age_range', 'patient_gender']:\n",
        "    data[col] = data[col].fillna('Unknown')\n",
        "\n",
        "data['findings'] = data['findings'].fillna('No findings provided')\n",
        "\n",
        "# Removing duplicates\n",
        "data = data.drop_duplicates()\n",
        "\n",
        "# Step 2: Standardizing Text Columns\n",
        "text_columns = ['diagnosis_category', 'diagnosis_sub_category', 'treatment_category',\n",
        "                'treatment_sub_category', 'determination', 'type', 'age_range',\n",
        "                'patient_gender', 'findings']\n",
        "\n",
        "for col in text_columns:\n",
        "    data[col] = data[col].str.strip().str.lower()\n",
        "\n",
        "# Step 3: Univariate Analysis\n",
        "categorical_columns = ['diagnosis_category', 'treatment_category', 'determination', 'type', 'age_range', 'patient_gender']\n",
        "\n",
        "for col in categorical_columns:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    data[col].value_counts().plot(kind='bar', title=f'Distribution of {col.capitalize()}', ylabel='Count', xlabel=col.capitalize())\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "# Step 4: Bivariate Analysis\n",
        "# Diagnosis Category vs Determination\n",
        "plt.figure(figsize=(14, 7))\n",
        "sns.countplot(data=data, x='diagnosis_category', hue='determination', order=data['diagnosis_category'].value_counts().index[:10])\n",
        "plt.title('Diagnosis Category vs Determination')\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylabel('Count')\n",
        "plt.xlabel('Diagnosis Category')\n",
        "plt.legend(title='Determination', loc='upper right')\n",
        "plt.show()\n",
        "\n",
        "# Age Range vs Patient Gender\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.countplot(data=data, x='age_range', hue='patient_gender', order=data['age_range'].value_counts().index)\n",
        "plt.title('Age Range vs Patient Gender')\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylabel('Count')\n",
        "plt.xlabel('Age Range')\n",
        "plt.legend(title='Patient Gender', loc='upper right')\n",
        "plt.show()\n",
        "\n",
        "# Step 5: Text Analysis\n",
        "# Tokenize and count word frequencies in the 'findings' column\n",
        "vectorizer = CountVectorizer(stop_words='english', max_features=20)\n",
        "word_counts = vectorizer.fit_transform(data['findings'])\n",
        "\n",
        "# Summarizing most frequent words\n",
        "word_frequency = pd.DataFrame({\n",
        "    'word': vectorizer.get_feature_names_out(),\n",
        "    'count': word_counts.toarray().sum(axis=0)\n",
        "}).sort_values(by='count', ascending=False)\n",
        "\n",
        "# Visualizing the most frequent words\n",
        "plt.figure(figsize=(10, 6))\n",
        "word_frequency.set_index('word').plot(kind='bar', legend=False, figsize=(10, 6))\n",
        "plt.title('Most Frequent Words in Findings')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xlabel('Word')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# Save the cleaned and processed dataset\n",
        "processed_file_path = 'processed_medical_review.csv'\n",
        "data.to_csv(processed_file_path, index=False)\n",
        "\n",
        "print(f\"Processed dataset saved to: {processed_file_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# 1. Memuat dataset\n",
        "file_path = '/content/processed_medical_review.csv'  # Ganti dengan path file Anda\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# 2. Analisis frekuensi kata untuk menemukan kata kunci yang sering muncul\n",
        "vectorizer = CountVectorizer(stop_words='english', max_features=50)\n",
        "X = vectorizer.fit_transform(data['findings'].dropna())\n",
        "word_freq = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "word_freq_sum = word_freq.sum().sort_values(ascending=False)\n",
        "print(\"Top 10 kata berdasarkan frekuensi:\")\n",
        "print(word_freq_sum.head(10))\n",
        "\n",
        "# 3. Menggunakan TF-IDF untuk mendapatkan kata kunci penting\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=50)\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(data['findings'].dropna())\n",
        "tfidf_terms = pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "tfidf_terms_sum = tfidf_terms.sum().sort_values(ascending=False)\n",
        "print(\"Top 10 kata berdasarkan skor TF-IDF:\")\n",
        "print(tfidf_terms_sum.head(10))\n",
        "\n",
        "# 4. Ekstraksi kata kunci pengobatan dari kolom \"findings\"\n",
        "treatment_keywords = [\n",
        "    'medication', 'drug', 'therapy', 'prescription', 'surgery',\n",
        "    'radiation', 'chemotherapy', 'hospitalization', 'procedure', 'diagnostic', 'therapy'\n",
        "]\n",
        "\n",
        "def extract_treatment_terms(text, keywords):\n",
        "    found_terms = []\n",
        "    for keyword in keywords:\n",
        "        if re.search(r'\\b' + keyword + r'\\b', text, re.IGNORECASE):\n",
        "            found_terms.append(keyword)\n",
        "    return ', '.join(found_terms) if found_terms else None\n",
        "\n",
        "# Terapkan ekstraksi pengobatan pada kolom 'findings'\n",
        "data['extracted_treatments'] = data['findings'].apply(lambda x: extract_treatment_terms(str(x), treatment_keywords))\n",
        "\n",
        "# 5. Menampilkan hasil ekstraksi pengobatan\n",
        "print(\"Hasil ekstraksi pengobatan:\")\n",
        "print(data[['reference_id', 'findings', 'extracted_treatments']].head())\n",
        "\n",
        "# 6. Membuat tabel pivot untuk menganalisis hubungan antara kategori diagnosis dan pengobatan\n",
        "pivot_treatment_diagnosis = pd.crosstab(data['diagnosis_category'], data['extracted_treatments'])\n",
        "\n",
        "# 7. Menampilkan hasil analisis hubungan diagnosis dengan pengobatan\n",
        "print(\"Hubungan antara diagnosis dan pengobatan:\")\n",
        "print(pivot_treatment_diagnosis)\n",
        "\n",
        "# 8. Menyimpan hasil analisis dalam file CSV (opsional)\n",
        "pivot_treatment_diagnosis.to_csv('/content/treatment_diagnosis_summary.csv')  # Ganti dengan path file tujuan\n",
        "\n",
        "# --- EDA (Exploratory Data Analysis) ---\n",
        "\n",
        "# 9. Visualisasi distribusi kata berdasarkan frekuensi (Word Frequency)\n",
        "plt.figure(figsize=(10, 6))\n",
        "word_freq_sum.head(20).plot(kind='bar', color='skyblue')\n",
        "plt.title('Top 20 Kata Berdasarkan Frekuensi')\n",
        "plt.xlabel('Kata')\n",
        "plt.ylabel('Frekuensi')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.show()\n",
        "\n",
        "# 10. Visualisasi kata penting berdasarkan skor TF-IDF\n",
        "plt.figure(figsize=(10, 6))\n",
        "tfidf_terms_sum.head(20).plot(kind='bar', color='lightcoral')\n",
        "plt.title('Top 20 Kata Berdasarkan Skor TF-IDF')\n",
        "plt.xlabel('Kata')\n",
        "plt.ylabel('Skor TF-IDF')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.show()\n",
        "\n",
        "# 11. Visualisasi distribusi kategori diagnosis\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(data=data, x='diagnosis_category', palette='Set2')\n",
        "plt.title('Distribusi Kategori Diagnosis')\n",
        "plt.xlabel('Kategori Diagnosis')\n",
        "plt.ylabel('Jumlah Kasus')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.show()\n",
        "\n",
        "# 12. Visualisasi hubungan diagnosis dengan pengobatan (treatment)\n",
        "# Menghitung frekuensi pengobatan per diagnosis\n",
        "treatment_counts = pivot_treatment_diagnosis.sum(axis=1).sort_values(ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "treatment_counts.head(20).plot(kind='bar', color='lightgreen')\n",
        "plt.title('Top 20 Diagnosis Berdasarkan Jumlah Pengobatan')\n",
        "plt.xlabel('Diagnosis')\n",
        "plt.ylabel('Jumlah Pengobatan')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "I5HhxRSeDSmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load dataset\n",
        "file_path = '/content/processed_medical_review.csv'  # Path in Google Colab\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# 1. **Text Analysis**: Word Frequency Analysis\n",
        "vectorizer = CountVectorizer(stop_words='english', max_features=50)  # Limit to 50 most common words\n",
        "X = vectorizer.fit_transform(data['findings'].dropna())\n",
        "word_freq = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "word_freq_sum = word_freq.sum().sort_values(ascending=False)\n",
        "# Display the top 10 words by frequency\n",
        "print(\"Top Word Frequency:\")\n",
        "print(word_freq_sum.head(10))\n",
        "\n",
        "# 2. **TF-IDF Analysis** (Identifying Key Phrases)\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=50)\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(data['findings'].dropna())\n",
        "tfidf_terms = pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "tfidf_terms_sum = tfidf_terms.sum().sort_values(ascending=False)\n",
        "# Display the top 10 TF-IDF terms\n",
        "print(\"Top TF-IDF Terms:\")\n",
        "print(tfidf_terms_sum.head(10))\n",
        "\n",
        "# 3. **Extract Treatment Terms** from 'findings' column\n",
        "treatment_keywords = ['medication', 'drug', 'therapy', 'prescription', 'surgery', 'radiation', 'chemotherapy', 'hospitalization', 'procedure', 'diagnostic']\n",
        "\n",
        "def extract_treatment_terms(text, keywords):\n",
        "    found_terms = []\n",
        "    for keyword in keywords:\n",
        "        if re.search(r'\\b' + keyword + r'\\b', text, re.IGNORECASE):\n",
        "            found_terms.append(keyword)\n",
        "    return ', '.join(found_terms) if found_terms else None\n",
        "\n",
        "data['extracted_treatments'] = data['findings'].apply(lambda x: extract_treatment_terms(str(x), treatment_keywords))\n",
        "# Display the first few rows with the extracted treatments\n",
        "print(\"Extracted Treatments:\")\n",
        "print(data[['reference_id', 'findings', 'extracted_treatments']].head())\n",
        "\n",
        "# 4. **Relationship between Diagnosis and Treatments**\n",
        "pivot_treatment_diagnosis = pd.crosstab(data['diagnosis_category'], data['extracted_treatments'])\n",
        "# Display the pivot table for treatment and diagnosis\n",
        "print(\"Treatment and Diagnosis Summary:\")\n",
        "print(pivot_treatment_diagnosis)\n",
        "\n",
        "# 5. **Analyzing Determination (Upheld vs. Overturned) based on Diagnosis and Treatment**\n",
        "filtered_determination = pd.crosstab(\n",
        "    data['determination'],\n",
        "    [data['diagnosis_category'], data['extracted_treatments']],\n",
        "    margins=True,  # Add totals to the table\n",
        "    margins_name=\"Total\"\n",
        ")\n",
        "# Display the determination distribution table\n",
        "print(\"Diagnosis and Treatment Distribution by Determination:\")\n",
        "print(filtered_determination)\n",
        "\n",
        "# -- EDA Section: Exploratory Data Analysis --\n",
        "\n",
        "# Basic Descriptive Statistics\n",
        "print(\"\\nBasic Descriptive Statistics:\")\n",
        "print(data.describe(include='all'))\n",
        "\n",
        "# Visualize Distribution of Determination (Upheld vs. Overturned)\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(x='determination', data=data, palette='Set2')\n",
        "plt.title('Distribution of Determination (Upheld vs. Overturned)')\n",
        "plt.xlabel('Determination')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "# Visualize Relationship Between Determination and Diagnosis Category\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(x='diagnosis_category', hue='determination', data=data, palette='Set1')\n",
        "plt.title('Determination Distribution by Diagnosis Category')\n",
        "plt.xlabel('Diagnosis Category')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EmLht_XfDTm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# 1. Load the dataset\n",
        "file_path = '/content/medicine.csv'  # Ganti dengan path file Anda\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# 2. Data Cleaning: Strip whitespaces and standardize text columns\n",
        "data['Drug_Name'] = data['Drug_Name'].str.strip().str.lower()  # Standardize Drug Name (upper case)\n",
        "data['Reason'] = data['Reason'].str.strip().str.title()  # Standardize Reason (title case)\n",
        "data['Description'] = data['Description'].str.strip().str.lower()  # Standardize Description (lower case)\n",
        "\n",
        "# 3. Remove duplicates\n",
        "data_cleaned = data.drop_duplicates(subset=['Drug_Name', 'Reason', 'Description'])\n",
        "\n",
        "# 4. Check for missing values (None expected in this case)\n",
        "missing_values = data_cleaned.isnull().sum()\n",
        "print(f\"Missing values: \\n{missing_values}\")\n",
        "\n",
        "# 5. EDA - Distribution of Drug Names (Top 10)\n",
        "top_drugs = data_cleaned['Drug_Name'].value_counts().head(10)\n",
        "\n",
        "# 6. EDA - Distribution of Reason (Top 10)\n",
        "top_reasons = data_cleaned['Reason'].value_counts().head(10)\n",
        "\n",
        "# 7. EDA - Length of Descriptions (word count per description)\n",
        "data_cleaned['Description_Length'] = data_cleaned['Description'].apply(lambda x: len(x.split()))\n",
        "\n",
        "# 8. Visualizations\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Drug Name Distribution\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.barplot(x=top_drugs.values, y=top_drugs.index, palette=\"viridis\")\n",
        "plt.title('Top 10 Most Common Drug Names')\n",
        "plt.xlabel('Frequency')\n",
        "plt.ylabel('Drug Name')\n",
        "\n",
        "# Reason Distribution\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.barplot(x=top_reasons.values, y=top_reasons.index, palette=\"plasma\")\n",
        "plt.title('Top 10 Most Common Reasons')\n",
        "plt.xlabel('Frequency')\n",
        "plt.ylabel('Reason')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Word Cloud of Most Frequent Terms in Description\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(data_cleaned['Description']))\n",
        "\n",
        "# Display the word cloud\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Word Cloud of Description Texts')\n",
        "plt.show()\n",
        "\n",
        "# Distribution of Description Lengths\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.histplot(data_cleaned['Description_Length'], kde=True, color='blue', bins=20)\n",
        "plt.title('Distribution of Description Lengths (Word Count)')\n",
        "plt.xlabel('Word Count')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# 9. Save the cleaned dataset to a CSV file\n",
        "cleaned_file_path = '/content/cleaned_medicine_data.csv'  # Ganti dengan path tempat Anda ingin menyimpan file\n",
        "data_cleaned.to_csv(cleaned_file_path, index=False)\n",
        "\n",
        "print(f\"Cleaned data saved to: {cleaned_file_path}\")"
      ],
      "metadata": {
        "id": "0jMfTvWpZvyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Membaca dataset yang sudah diupload\n",
        "file_path = '/content/cleaned_medicine_data.csv'  # Sesuaikan dengan path file\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# 2. Feature Engineering\n",
        "\n",
        "# 2.1 Panjang Nama Obat\n",
        "data['Drug_Name_Length'] = data['Drug_Name'].apply(len)\n",
        "\n",
        "# 2.2 Jumlah Kata dalam Nama Obat\n",
        "data['Drug_Name_Word_Count'] = data['Drug_Name'].apply(lambda x: len(x.split()))\n",
        "\n",
        "# 2.3 Jumlah Kata dalam Deskripsi\n",
        "data['Description_Length'] = data['Description'].apply(lambda x: len(x.split()))\n",
        "\n",
        "# 2.4 Kategori Alasan (contoh: \"Acne\", \"Pain\", atau lainnya)\n",
        "data['Reason_Category'] = data['Reason'].apply(lambda x: 'Acne' if 'Acne' in x else ('Pain' if 'Pain' in x else 'Other'))\n",
        "\n",
        "# 2.5 Kehadiran Kata \"Acne\" dalam Deskripsi\n",
        "data['Contains_Acne'] = data['Description'].apply(lambda x: 1 if 'acne' in x.lower() else 0)\n",
        "\n",
        "# 2.7 Rasio Nama Obat terhadap Deskripsi\n",
        "data['Name_to_Description_Ratio'] = data['Drug_Name_Length'] / data['Description_Length']\n",
        "\n",
        "# Tampilkan beberapa baris pertama untuk melihat fitur baru yang ditambahkan\n",
        "print(data.head())\n",
        "\n",
        "# 3. Eksplorasi Data (EDA) dan Visualisasi\n",
        "\n",
        "# Set up a large plot size\n",
        "plt.figure(figsize=(16, 10))\n",
        "\n",
        "# 3.1 Distribusi Panjang Nama Obat\n",
        "plt.subplot(2, 3, 1)\n",
        "sns.histplot(data['Drug_Name_Length'], kde=True, color='blue', bins=20)\n",
        "plt.title('Distribusi Panjang Nama Obat')\n",
        "plt.xlabel('Panjang Nama Obat')\n",
        "plt.ylabel('Frekuensi')\n",
        "\n",
        "# 3.2 Distribusi Jumlah Kata dalam Nama Obat\n",
        "plt.subplot(2, 3, 2)\n",
        "sns.histplot(data['Drug_Name_Word_Count'], kde=True, color='green', bins=20)\n",
        "plt.title('Distribusi Jumlah Kata dalam Nama Obat')\n",
        "plt.xlabel('Jumlah Kata dalam Nama Obat')\n",
        "plt.ylabel('Frekuensi')\n",
        "\n",
        "# 3.3 Distribusi Jumlah Kata dalam Deskripsi\n",
        "plt.subplot(2, 3, 3)\n",
        "sns.histplot(data['Description_Length'], kde=True, color='orange', bins=20)\n",
        "plt.title('Distribusi Jumlah Kata dalam Deskripsi')\n",
        "plt.xlabel('Jumlah Kata dalam Deskripsi')\n",
        "plt.ylabel('Frekuensi')\n",
        "\n",
        "# 3.4 Distribusi Rasio Nama Obat terhadap Deskripsi\n",
        "plt.subplot(2, 3, 4)\n",
        "sns.histplot(data['Name_to_Description_Ratio'], kde=True, color='purple', bins=20)\n",
        "plt.title('Distribusi Rasio Nama Obat terhadap Deskripsi')\n",
        "plt.xlabel('Rasio Nama Obat terhadap Deskripsi')\n",
        "plt.ylabel('Frekuensi')\n",
        "\n",
        "# 3.6 Distribusi Kategori Alasan\n",
        "plt.subplot(2, 3, 5)\n",
        "sns.countplot(x='Reason_Category', data=data, palette='Set2')\n",
        "plt.title('Distribusi Kategori Alasan Penggunaan Obat')\n",
        "plt.xlabel('Kategori Alasan')\n",
        "plt.ylabel('Jumlah')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 3.7 Heatmap untuk Korelasi Fitur Numerik\n",
        "corr_matrix = data[['Drug_Name_Length', 'Drug_Name_Word_Count', 'Description_Length', 'Contains_Acne', 'Name_to_Description_Ratio']].corr()\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
        "plt.title('Heatmap Korelasi Fitur Numerik')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "asnU_r3JZ0X2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load datasets\n",
        "processed_medical_review_path = '/content/processed_medical_review.csv'\n",
        "cleaned_medicine_data_path = '/content/cleaned_medicine_data.csv'\n",
        "\n",
        "processed_medical_review_df = pd.read_csv(processed_medical_review_path)\n",
        "cleaned_medicine_data_df = pd.read_csv(cleaned_medicine_data_path)\n",
        "\n",
        "# Initialize TF-IDF Vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit the TF-IDF vectorizer on the 'diagnosis_sub_category' and 'Reason' columns\n",
        "tfidf_diagnosis = vectorizer.fit_transform(processed_medical_review_df['diagnosis_sub_category'])\n",
        "tfidf_reasons = vectorizer.transform(cleaned_medicine_data_df['Reason'])\n",
        "\n",
        "# Calculate cosine similarity between all diagnosis sub-categories and reasons\n",
        "cosine_similarities = cosine_similarity(tfidf_diagnosis, tfidf_reasons)\n",
        "\n",
        "# Now, let's find the maximum similarity per diagnosis sub-category\n",
        "matching_values = []\n",
        "for i, diagnosis in enumerate(processed_medical_review_df['diagnosis_sub_category']):\n",
        "    # Get the index of the most similar reason\n",
        "    best_match_idx = cosine_similarities[i].argmax()\n",
        "\n",
        "    # Get the similarity score\n",
        "    best_match_score = cosine_similarities[i][best_match_idx]\n",
        "\n",
        "    # Only add if the similarity score is above a threshold (e.g., 0.2)\n",
        "    if best_match_score > 0.2:  # You can adjust this threshold as needed\n",
        "        matching_values.append((diagnosis, cleaned_medicine_data_df.iloc[best_match_idx]['Reason'], best_match_score))\n",
        "\n",
        "# Display the matching diagnosis and reasons (with similarity score)\n",
        "matching_values\n",
        "\n",
        "# Function to find matching medicines based on user input\n",
        "def find_matching_medicines(diagnosis_input):\n",
        "    # Match input to reason column\n",
        "    matching_reasons = cleaned_medicine_data_df[cleaned_medicine_data_df['Reason'].str.lower() == diagnosis_input.lower()]\n",
        "\n",
        "    # Check if any matching reasons found\n",
        "    if not matching_reasons.empty:\n",
        "        # Return list of drug names that match the diagnosis\n",
        "        return matching_reasons['Drug_Name'].tolist()\n",
        "    else:\n",
        "        return []\n",
        "\n",
        "# Simulate user input for diagnosis\n",
        "def get_user_input_with_matching_values():\n",
        "    # Simulate a user input (you can replace this part with actual input for real testing)\n",
        "    diagnosis_input = input(\"Please enter your diagnosis from the list above: \")  # For real use, use input() here\n",
        "\n",
        "    print(f\"User input: {diagnosis_input}\")\n",
        "\n",
        "    # Find matching medicines for the given input\n",
        "    matching_medicines = find_matching_medicines(diagnosis_input)\n",
        "\n",
        "    if matching_medicines:\n",
        "        print(\"\\nMedicines that match your diagnosis:\")\n",
        "        for i, medicine in enumerate(matching_medicines, 1):\n",
        "            print(f\"{i}. {medicine}\")\n",
        "    else:\n",
        "        print(\"No matching drugs found for your diagnosis.\")\n",
        "\n",
        "# Run the simulation\n",
        "get_user_input_with_matching_values()"
      ],
      "metadata": {
        "id": "pmQEWdAPaaqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZO-5oc1YHB3c"
      }
    }
  ]
}